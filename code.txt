## create train, test and valid generators
batch_size=40 # We will use and EfficientetB3 model, with image size of (300,233) this size should not cause resource error
trgen=ImageDataGenerator(horizontal_flip=True,rotation_range=20, width_shift_range=.2,
                                  height_shift_range=.2, zoom_range=.2 )
t_and_v_gen=ImageDataGenerator()
train_gen=trgen.flow_from_dataframe(train_df, x_col='filepaths', y_col='labels', target_size=img_size,
                                   class_mode='categorical', color_mode='rgb', shuffle=True, batch_size=batch_size)
valid_gen=t_and_v_gen.flow_from_dataframe(valid_df, x_col='filepaths', y_col='labels', target_size=img_size,
                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=batch_size)
# for the test_gen we want to calculate the batch size and test steps such that batch_size X test_steps= number of samples in test set
# this insures that we go through all the sample in the test set exactly once.
length=len(test_df)
test_batch_size=sorted([int(length/n) for n in range(1,length+1) if length % n ==0 and length/n<=80],reverse=True)[0]  
test_steps=int(length/test_batch_size)
test_gen=t_and_v_gen.flow_from_dataframe(test_df, x_col='filepaths', y_col='labels', target_size=img_size,
                                   class_mode='categorical', color_mode='rgb', shuffle=False, batch_size=test_batch_size)
# from the generator we can get information we will need later
classes=list(train_gen.class_indices.keys())
class_indices=list(train_gen.class_indices.values())
class_count=len(classes)
labels=test_gen.labels
print ( 'test batch size: ' ,test_batch_size, '  test steps: ', test_steps, ' number of classes : ', class_count)
print ('{0:^25s}{1:^12s}'.format('class name', 'class index'))
for klass, index in zip(classes, class_indices):
    print(f'{klass:^25s}{str(index):^12s}')

## create a model using transfer learnimg using EfficientNetB3 as the base model
# Define image shape and model name
img_shape = (img_size[0], img_size[1], 3)
model_name = 'EfficientNetB5'

# Load the base model with EfficientNetB5
base_model = tf.keras.applications.efficientnet.EfficientNetB5(
    include_top=False,
    weights="imagenet",
    input_shape=img_shape,
    pooling='max'
)

# Make the base model trainable
base_model.trainable = True

# Add custom layers on top of the base model
x = base_model.output
x = BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001)(x)
x = Dense(1024, 
          kernel_regularizer=regularizers.l2(0.016),
          activity_regularizer=regularizers.l1(0.006),
          bias_regularizer=regularizers.l1(0.006),
          activation='relu')(x)
x = Dropout(rate=0.3, seed=123)(x)
x = Dense(128, 
          kernel_regularizer=regularizers.l2(0.016),
          activity_regularizer=regularizers.l1(0.006),
          bias_regularizer=regularizers.l1(0.006),
          activation='relu')(x)
x = Dropout(rate=0.45, seed=123)(x)
output = Dense(class_count, activation='softmax')(x)

# Create the final model
model = Model(inputs=base_model.input, outputs=output)

# Compile the model
lr = 0.001  # Start with this learning rate
model.compile(optimizer=Adamax(learning_rate=lr), 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

## define a custom callback to query user to either halt or continue training for N more epochs

class ASK(tf.keras.callbacks.Callback):
    def __init__(self, epochs, ask_epoch):  # Initialization of the callback
        super(ASK, self).__init__()
        self.ask_epoch = ask_epoch
        self.epochs = epochs
        self.ask = True  # If True, query the user on a specified epoch
        
    def on_train_begin(self, logs=None):  # This runs at the beginning of training
        if self.ask_epoch == 0: 
            print('You set ask_epoch = 0, ask_epoch will be set to 1', flush=True)
            self.ask_epoch = 1
        if self.ask_epoch >= self.epochs:  # If ask_epoch is greater than the number of epochs
            print(f'ask_epoch >= epochs, will train for {self.epochs} epochs', flush=True)
            self.ask = False  # Do not query the user
        if self.epochs == 1:
            self.ask = False  # Running only for 1 epoch, so do not query user
        else:
            print(f'Training will proceed until epoch {self.ask_epoch}, then you will be asked to')
            print('enter H to halt training or enter an integer for how many more epochs to run, then be asked again')
        self.start_time = time.time()  # Set the time at which training started
        
    def on_train_end(self, logs=None):  # Runs at the end of training
        tr_duration = time.time() - self.start_time  # Determine how long the training cycle lasted
        hours = tr_duration // 3600
        minutes = (tr_duration - (hours * 3600)) // 60
        seconds = tr_duration - ((hours * 3600) + (minutes * 60))
        msg = f'Training elapsed time was {int(hours)} hours, {int(minutes)} minutes, {seconds:.2f} seconds'
        print(msg, flush=True)  # Print out training duration time
        
    def on_epoch_end(self, epoch, logs=None):  # Method runs at the end of each epoch
        if self.ask:  # Are the conditions right to query the user?
            if epoch + 1 == self.ask_epoch:  # Is this epoch the one for querying the user?
                print('\nEnter H to end training or an integer for the number of additional epochs to run, then ask again')
                ans = input()
                
                if ans in ['H', 'h', '0']:  # Quit training for these conditions
                    print(f'You entered {ans}. Training halted on epoch {epoch + 1} due to user input\n', flush=True)
                    self.model.stop_training = True  # Halt training
                else:  # User wants to continue training
                    self.ask_epoch += int(ans)
                    if self.ask_epoch > self.epochs:
                        print(f'\nYou specified maximum epochs as {self.epochs}, cannot train for {self.ask_epoch}', flush=True)
                    else:
                        print(f'You entered {ans}. Training will continue to epoch {self.ask_epoch}', flush=True)

## Define 2 useful Keras callbacks to control learning rate and early stopping. Instantiate the ASK callback
epochs = 40
ask_epoch = 5
ask = ASK(epochs, ask_epoch)
rlronp = tf.keras.callbacks.ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=2, verbose=1)
estop = tf.keras.callbacks.EarlyStopping(monitor="val_loss", patience=4, verbose=1, restore_best_weights=True)
callbacks = [rlronp, estop, ask]

## 
history=model.fit(x=train_gen,  epochs=epochs, verbose=1, callbacks=callbacks,  validation_data=valid_gen,
               validation_steps=None,  shuffle=False,  initial_epoch=0)